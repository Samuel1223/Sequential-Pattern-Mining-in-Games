# -*- coding: utf-8 -*-
"""modeling_for_login_and_charge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14LIpYnU1LoX-POxJh0OXSMH5Vn_uB7Mf

# Login log
"""

from google.colab import drive
drive.mount('/content/drive')

import pymongo

import pandas as pd
import numpy as np
import os
import warnings

import time
from datetime import timedelta

import matplotlib.pyplot as plt
import plotly.graph_objects as go
import seaborn as sns
from pandas.plotting import radviz, andrews_curves, parallel_coordinates

from imblearn.over_sampling import SMOTE, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingRegressor, StackingClassifier
from sklearn.svm import SVC, NuSVC, LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier

from xgboost import XGBClassifier, XGBRFRegressor, plot_importance

from lightgbm import LGBMClassifier, LGBMRegressor

import pandas as pd
import numpy as np
import warnings
from datetime import timedelta
import tqdm

warnings.filterwarnings("ignore")

def Changetime(str1):
    Unixtime = int(time.mktime(time.strptime(str1, '%Y-%m-%d %H:%M:%S')))
    return Unixtime

# 破產後包點擊行為處理程式(Y)
def BrokenPlayerDepositClickTarget_ETL(BrokenPlayerDepositClickTarget):
    output = BrokenPlayerDepositClickTarget.copy()
    output['broken_time_end'] = output['broken_time_end'].str[:19]
    output['broken_time_start'] = output['broken_time_start'].str[:19]
    output['broken_time_clickbrokenpackage_end'] = output['broken_time_clickbrokenpackage_end'].str[:19]

    # 排除 2 小時內再度破產的資料(破產禮包CD時間 2 hr, 中間無論破產幾次都不會跳出)
    CD_time = 2*60*60
    output['unix_time_cost'] = CD_time + 1
    for i in range(len(output)):
        try:
            if (output.loc[i, 'accountid'] == output.loc[i-1, 'accountid']):
                output.loc[i,'unix_time_cost'] = Changetime(output.loc[i, 'broken_time_end']) - Changetime(output.loc[i-1, 'broken_time_end'])
            else:
                pass
        except:
            pass

    output = output[output['unix_time_cost'] > CD_time].reset_index().drop(['index','unix_time_cost'], axis=1)

    return output
def login_log_preprocessing(login_log,y):
    login_log['createtime'] = pd.to_datetime(login_log['createtime'])
    login_log['broken_time_end'] = pd.to_datetime(login_log['broken_time_end'])
    login_log['hour'] = login_log['createtime'].dt.hour
    login_log['morning_log'] = login_log['hour'].apply(lambda x : 6 < x < 12)
    login_log['afternoon_log'] = login_log['hour'].apply(lambda x : 12 <= x < 17)
    login_log['night_log'] = login_log['hour'].apply(lambda x : 17 <= x < 22)
    login_log['midnight_log'] = login_log['hour'].apply(lambda x : 22 <= x <= 24 or  x <= 6)
    
    final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)
    
    temp =pd.pivot_table(login_log,index = ['accountid','broken_time_end'],aggfunc = 'sum')[['morning_log','afternoon_log','night_log','midnight_log']].reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp = pd.pivot_table(login_log,index = ['accountid','broken_time_end'],aggfunc = 'count')['afternoon_log'].reset_index().rename(columns = {'afternoon_log' : 'count'})
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp = pd.get_dummies(data = login_log[['accountid','broken_time_end','encate']],columns = ['encate'])
    temp = pd.pivot_table(temp,index = ['accountid','broken_time_end'],aggfunc = 'sum').add_prefix('sum_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp_ = login_log[login_log['createtime'] < login_log['broken_time_end'] - timedelta(hours = 2)]
    
    temp = pd.get_dummies(data = temp_[['accountid','broken_time_end','encate']],columns = ['encate'])
    temp = pd.pivot_table(temp,index = ['accountid','broken_time_end'],aggfunc = 'sum').add_prefix('broken_former_2hrs_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp = pd.pivot_table(temp_,index =['accountid','broken_time_end'],aggfunc =  'sum')[['morning_log','afternoon_log','night_log','midnight_log']].add_prefix('broken_former_2hrs_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    final_table.fillna(0,inplace = True)
    return final_table

def charge_preprocessing(charge,y):
    charge['createtime'] = pd.to_datetime(charge['createtime'])
    charge['broken_time_end'] = pd.to_datetime(charge['broken_time_end'])
    charge['hour']  = charge['createtime'].dt.hour
    charge['morning_log'] = charge['hour'].apply(lambda x : 6 < x < 12)
    charge['afternoon_log'] = charge['hour'].apply(lambda x : 12 <= x < 17)
    charge['night_log'] = charge['hour'].apply(lambda x : 17 <= x < 22)
    charge['midnight_log'] = charge['hour'].apply(lambda x : 22 <= x <= 24 or  x <= 6)

    final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)

    temp = pd.pivot_table(charge,index = ['accountid','broken_time_end'],aggfunc = 'sum')[['morning_log','afternoon_log','night_log','midnight_log','amount','money']].add_prefix('sum_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')

    temp = pd.pivot_table(charge,index = ['accountid','broken_time_end'],aggfunc = 'mean')[['amount','money']].add_prefix('mean_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')

    temp = pd.pivot_table(charge,index = ['accountid','broken_time_end'],aggfunc = 'max')[['amount','money']].add_prefix('max_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp_ = charge[charge['createtime'] < charge['broken_time_end'] - timedelta(hours = 2)]
    
    temp = pd.pivot_table(temp_,index = ['accountid','broken_time_end'],aggfunc = 'max')[['amount','money']].add_prefix('broken_former_2hrs_max_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    temp = pd.pivot_table(temp_,index = ['accountid','broken_time_end'],aggfunc = 'sum')[['morning_log','afternoon_log','night_log','midnight_log','amount','money']].add_prefix('broken_former_2hrs_sum_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    
    final_table.fillna(0,inplace=True)
    return final_table

def data_balance(X_train, Y_train, method='SMOTE'):

    print('data balance by:')

    if method == 'UnderSample':
        # under sampling
        print('UnderSample')
        X_under, Y_under = RandomUnderSampler(sampling_strategy='majority').fit_resample(X_train, Y_train)
        # output = pd.concat([X_under, Y_under], axis=1)
        return X_under, Y_under

    elif method == 'SMOTE':
        # SMOTE
        print('SMOTE')
        X_smo, Y_smo = SMOTE(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_smo, Y_smo], axis=1)
        return X_smo, Y_smo

    elif method == 'BorderlineSMOTE':
        # BorderlineSMOTE
        print('BorderlineSMOTE')
        X_blsmo, Y_blsmo = BorderlineSMOTE(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_blsmo, Y_blsmo], axis=1)
        return X_blsmo, Y_blsmo

    elif method == 'SMOTETomek':
        # SMOTETomek
        print('SMOTETomek')
        X_smot, Y_smot = SMOTETomek(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_smot, Y_smot], axis=1)
        return X_smot, Y_smot
def classifier_score(model, X_test, Y_test):

    accuracy = round(accuracy_score(model.predict(X_test), Y_test),2)
    precision = round(precision_score(model.predict(X_test), Y_test, average='macro'),2)
    recall = round(recall_score(model.predict(X_test), Y_test, average='macro'),2)
    f1 = round(f1_score(model.predict(X_test), Y_test, average='macro'),2)
    roc_auc = round(roc_auc_score(model.predict(X_test), Y_test),2)

    return accuracy, precision, recall, f1 ,roc_auc
def train_classifier_model(X_train, X_test, Y_train, Y_test):

    # Classifier model
    print('models:')
    
    # decisiontree
    clf_dt = DecisionTreeClassifier(max_depth=8)
    clf_dt.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_dt, X_test, Y_test)
    print(f'decisiontree => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    
    # randomforest
    clf_rf = RandomForestClassifier()
    clf_rf.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc= classifier_score(clf_rf, X_test, Y_test)
    print(f'randomforest => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    
    # xgboost
    clf_xgb = XGBClassifier()
    clf_xgb.fit(X_train,Y_train, verbose=False)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_xgb, X_test, Y_test)
    print(f'xgboost => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # extratrees
    clf_ext = ExtraTreesClassifier()
    clf_ext.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_ext, X_test, Y_test)
    print(f'extratrees => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # adaboost
    clf_adb = AdaBoostClassifier()
    clf_adb.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_adb, X_test, Y_test)
    print(f'adaboost => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # lgbm
    clf_lgbm = LGBMClassifier()
    clf_lgbm.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_lgbm, X_test, Y_test)
    print(f'lgbm => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # gradientboosting
    clf_gdb = GradientBoostingClassifier()
    clf_gdb.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_gdb, X_test, Y_test)
    print(f'gradientboosting => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # bagging
    clf_bag = BaggingClassifier()
    clf_bag.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_bag, X_test, Y_test)
    print(f'bagging => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # knn
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_knn, X_test, Y_test)
    print(f'knn => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # NuSVC
    #clf_nsv = NuSVC()
    #clf_nsv.fit(X_train,Y_train)
    #accuracy, precision, recall, f1, roc_auc = classifier_score(clf_nsv, X_test, Y_test)
    #print(f'nusvc => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    clf_nsv = ''
    return clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv

# string 2 unixtime function
def Changetime(str1):
    Unixtime = int(time.mktime(time.strptime(str1, '%Y-%m-%d %H:%M:%S')))
    return Unixtime

# 破產後包點擊行為處理程式(Y)
def BrokenPlayerDepositClickTarget_ETL(BrokenPlayerDepositClickTarget):
    output = BrokenPlayerDepositClickTarget.copy()
    output['broken_time_end'] = output['broken_time_end'].astype(str)
    output['broken_time_end'] = output['broken_time_end'].str[:19]
    output['broken_time_start'] = output['broken_time_start'].str[:19]
    output['broken_time_clickbrokenpackage_end'] = output['broken_time_clickbrokenpackage_end'].str[:19]

    # 排除 2 小時內再度破產的資料(破產禮包CD時間 2 hr, 中間無論破產幾次都不會跳出)
    CD_time = 2*60*60
    output['unix_time_cost'] = CD_time + 1
    for i in range(len(output)):
        try:
            if (output.loc[i, 'accountid'] == output.loc[i-1, 'accountid']):
                output.loc[i,'unix_time_cost'] = Changetime(output.loc[i, 'broken_time_end']) - Changetime(output.loc[i-1, 'broken_time_end'])
            else:
                pass
        except:
            pass

    output = output[output['unix_time_cost'] > CD_time].reset_index().drop(['index','unix_time_cost'], axis=1)

    return output

# gameinfo log 統計量資料前處理程式(X)
def BrokenPlayer_GameInfo_FeatureExtraction(BrokenPlayerGameInfo, Themeid_to_cate):
    # 與遊戲分類表合併
    output = pd.merge(BrokenPlayerGameInfo,Themeid_to_cate,left_on='themeid',right_on='Themeid').drop(['Themeid'], axis=1)
    output.rename(columns = {'en_cate':'GameType'},inplace = True)

    # 時間格式正規化
    output['broken_time_end'] = output['broken_time_end'].str[:19]
    output['broken_time_start'] = output['broken_time_start'].str[:19]
    output['createtime'] = output['createtime'].str[:19]

    # # 時間格式轉換 unix time
    # output['createtime_unix'] = list(map(lambda x : Changetime(x), output['createtime']))
    # output['broken_time_end_unix'] = list(map(lambda x : Changetime(x), output['broken_time_end']))
    # output['broken_delay'] = output['broken_time_end_unix'] - output['createtime_unix']
    # # 破產前蒐集的時間範圍
    # output = output[output['broken_delay'] <= BrokeDelayHour*60*60].reset_index().drop('index', axis=1)

    # 留破產時間做後續聚合
    # output = output.drop(['broken_time_start','createtime','createtime_unix','broken_time_end_unix','broken_delay'], axis=1)
    output = output.drop(['broken_time_start','createtime'], axis=1)

    # 不同遊戲點擊次數及額度不同(ex. 捕魚遊戲非常高), 暫不考慮極短時間的變化
    output = output.drop(['round'], axis=1)

    # 房間等級轉換, 分為"其他 other","休閒 leisure","正式尊榮 honor","正式富豪 rich"
    output['RoomType'] = 'Other'
    output['RoomType'][output['themeid'].astype(str).str[-1] == '4'] = 'Leisure'
    output['RoomType'][(output['themeid'].astype(str).str[-1] == '1') & (output['roomindex'] > 500)] = 'Honor'
    output['RoomType'][(output['themeid'].astype(str).str[-1] == '1') & (output['roomindex'] <= 500)] = 'Rich'
    output = output.drop(['themeid', 'roomindex'], axis=1)

    # 轉換 bet, win 為統一標準(紅鑽)
    output['TrueBet'] = output['bet'] * output['changerate']
    output['TrueWin'] = output['win'] * output['changerate']
    output = output.drop(['bet', 'win', 'changerate'], axis=1)

    # 真實獲利
    output['Profit'] = output['TrueWin'] - output['TrueBet']

    # 對局勝利與否
    output['WinGame'] = 0
    output['WinGame'][(output['TrueWin'] - output['TrueBet']) > 0] = 1

    # 道具使用與否
    output['Props'] = 0
    output['Props'][output['TrueBet'] == 0] = 1

    # 局數計數欄位
    output['GamesCount'] = 1

    # One-Hot Encoding
    output_dum = pd.get_dummies(output[['GameType','RoomType']])
    output = pd.concat([output,output_dum], axis=1)
    output = output.drop(['GameType','RoomType'], axis=1)

    # 遊戲種類獲利
    output['GameType_ARCADE'] = output['GameType_ARCADE'] * output['Profit']
    output['GameType_CASINO'] = output['GameType_CASINO'] * output['Profit']
    output['GameType_PACHISLOT'] = output['GameType_PACHISLOT'] * output['Profit']
    output['GameType_POKER'] = output['GameType_POKER'] * output['Profit']
    output['GameType_SLOT'] = output['GameType_SLOT'] * output['Profit']

    # 房間種類獲利
    output['RoomType_Honor'] = output['RoomType_Honor'] * output['Profit']
    output['RoomType_Leisure'] = output['RoomType_Leisure'] * output['Profit']
    output['RoomType_Other'] = output['RoomType_Other'] * output['Profit']
    output['RoomType_Rich'] = output['RoomType_Rich'] * output['Profit']

    # 統計量加總欄位
    output_static = output.groupby(['accountid','broken_time_end']).sum().reset_index()

    # 去掉序號欄位
    output_static = output_static.drop(['row_number'], axis=1)

    # 返獎率
    output_static['ReturnRate'] = 1 + output_static['Profit'] / output_static['TrueBet']

    # 對局勝率
    output_static['WinRatio'] = output_static['WinGame'] / output_static['GamesCount']
    output_static = output_static.drop(['WinGame'], axis=1)

    # 道具使用率
    output_static['PropsRatio'] = output_static['Props'] / output_static['GamesCount']
    output_static = output_static.drop(['Props'], axis=1)    

    # 遊戲種類獲利貢獻佔比
    output_static['GameARCADE_ReturnShare'] = output_static['GameType_ARCADE'] / output_static['TrueBet']
    output_static['GameCASINO_ReturnShare'] = output_static['GameType_CASINO'] / output_static['TrueBet']
    output_static['GamePACHISLOT_ReturnShare'] = output_static['GameType_PACHISLOT'] / output_static['TrueBet']
    output_static['GamePOKER_ReturnShare'] = output_static['GameType_POKER'] / output_static['TrueBet']
    output_static['GameSLOT_ReturnShare'] = output_static['GameType_SLOT'] / output_static['TrueBet']
    output_static = output_static.drop(['GameType_ARCADE','GameType_CASINO','GameType_PACHISLOT','GameType_POKER','GameType_SLOT'], axis=1)

    # 房間種類獲利貢獻佔比
    output_static['RoomHonor_ReturnShare'] = output_static['RoomType_Honor'] / output_static['TrueBet']
    output_static['RoomLeisure_ReturnShare'] = output_static['RoomType_Leisure'] / output_static['TrueBet']
    output_static['RoomOther_ReturnShare'] = output_static['RoomType_Other'] / output_static['TrueBet']
    output_static['RoomRich_ReturnShare'] = output_static['RoomType_Rich'] / output_static['TrueBet']
    output_static = output_static.drop(['RoomType_Honor','RoomType_Leisure','RoomType_Other','RoomType_Rich'], axis=1)

    # 替換掉無限值
    output_static = output_static.replace(np.inf, 0)

    return output, output_static

# depositclick log 統計量資料前處理程式(X)
def BrokenPlayer_DepositClick_FeatureExtraction(BrokenPlayerDepositClick):

    output = BrokenPlayerDepositClick.copy()

    # 時間格式正規化
    output['broken_time_end'] = output['broken_time_end'].str[:19]
    output['broken_time_start'] = output['broken_time_start'].str[:19]
    output['createtime'] = output['createtime'].str[:19]

    # # 時間格式轉換 unix time
    # output['createtime_unix'] = list(map(lambda x : Changetime(x), output['createtime']))
    # output['broken_time_end_unix'] = list(map(lambda x : Changetime(x), output['broken_time_end']))
    # output['broken_delay'] = output['broken_time_end_unix'] - output['createtime_unix']
    # # 破產前蒐集的時間範圍
    # output = output[output['broken_delay'] <= BrokeDelayHour*60*60].reset_index().drop('index', axis=1)

    # 留破產時間做後續聚合
    # output = output.drop(['broken_time_start','createtime','createtime_unix','broken_time_end_unix','broken_delay'], axis=1)
    output = output.drop(['broken_time_start','createtime'], axis=1)

    # 相關欄位僅留一個表示(producttype, description, productid)
    output = output.drop(['description', 'productid'], axis=1)
    output = output.drop(['themename'], axis=1)

    # 將類別型欄位轉成 str type(方便後面 one hot)
    output[['userclienttype','itemname']] = output[['userclienttype','itemname']].astype(str)

    # 遊戲類別欄位將空值轉換成 other
    output['en_cate'] = output['en_cate'].fillna('OTHER')

    # 將禮包類型細類除了0以外的種類轉成1(類別太多避免為度爆炸)
    output['producttype'][output['producttype'] != 0] = 1

    # 點擊位置只保留對應表的類別,其他歸類為 0
    output['entrance'] = output['entrance'].str.strip()
    # 二分法(避免維度過大)
    output['entrance'][~(output['entrance'].isin([str(x) for x in list(range(0,23))]))] = 0
    output['entrance'][output['entrance'].isin([str(x) for x in list(range(0,23))])] = 1
    output['entrance'] = output['entrance'].astype(int)
    
    # One-Hot Encoding
    output_dum = pd.get_dummies(output[['userclienttype','en_cate','itemname']])
    output = pd.concat([output,output_dum], axis=1)
    output = output.drop(['userclienttype','en_cate','itemname'], axis=1)

    # 統計量 sum 欄位
    output_static_sum = output.drop(['viplevel','amount','prize'], axis=1).groupby(['accountid','broken_time_end']).sum().reset_index()
    # 統計量 avg 欄位
    output_static_avg = output[['accountid','broken_time_end','viplevel','amount','prize']].groupby(['accountid','broken_time_end']).mean().reset_index()
    output_static_avg.rename(columns = {'viplevel':'avg_viplevel','amount':'avg_amount','prize':'avg_prize'},inplace = True)
    # 統計量 max 欄位
    output_static_max = output[['accountid','broken_time_end','amount','prize']].groupby(['accountid','broken_time_end']).max().reset_index()
    output_static_max.rename(columns = {'amount':'max_amount','prize':'max_prize'},inplace = True)
    # 統計量 min 欄位
    output_static_min = output[['accountid','broken_time_end','amount','prize']].groupby(['accountid','broken_time_end']).min().reset_index()
    output_static_min.rename(columns = {'amount':'min_amount','prize':'min_prize'},inplace = True)
    # 統計量欄位合併
    output_static = pd.merge(output_static_sum, output_static_avg,left_on=['accountid','broken_time_end'], right_on=['accountid','broken_time_end'], how="inner")
    output_static = pd.merge(output_static, output_static_max,left_on=['accountid','broken_time_end'], right_on=['accountid','broken_time_end'], how="inner")
    output_static = pd.merge(output_static, output_static_min,left_on=['accountid','broken_time_end'], right_on=['accountid','broken_time_end'], how="inner")

    return output, output_static
def tree_model_explainer(model, df):

    shap.initjs

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(df)
    print('expect value', explainer.expected_value)

    # 輸出 feature importance 排序
    feature_importance = pd.DataFrame()
    feature_importance['feature'] = df.columns
    feature_importance['importance'] = np.abs(shap_values).mean(0)
    feature_importance = feature_importance.sort_values('importance', ascending=False)

    # shap value 絕對值排名
    shap.summary_plot(shap_values, df, plot_type='bar')

    # shap value
    shap.summary_plot(shap_values, df, plot_type='dot')

    # 分別看特徵值與預測影響之關係圖(top5)
    feature_importance_top5 = feature_importance[:5]['feature'].to_list()
    print('feature_importance_top5', feature_importance_top5)
    for feature in feature_importance_top5:
        shap.dependence_plot(ind=feature,shap_values=shap_values, features=df, interaction_index=None)

    # top2 特徵值與預測影響之關係圖交互影響
    shap.dependence_plot(ind=feature_importance_top5[0],shap_values=shap_values, features=df, interaction_index=feature_importance_top5[1])

# Stacking function
def Ensemble_Stacking(X_train, X_test, Y_train, Y_test, list_model:list, final_model):

    #設立模型參數
    clf = StackingClassifier(estimators=list_model, final_estimator=final_model)

    #訓練 Stacking 模型
    model = clf.fit(X_train, Y_train)

    #model.score
    classifier_score(model, X_test, Y_test)
    accuracy, precision, recall, f1, roc_auc = classifier_score(model, X_test, Y_test)
    print(f'stacking => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    print("Stack_Model_Score : "+str(model.score(X_test, Y_test)))
    print("Estimators_Model : "+str(list_model))
    print("Final_Estimator_Model : "+str(final_model))

    
    return model
def plot_importance(clf_rf,input_columns):
    import matplotlib.pyplot as plt
    feat_importances = pd.Series(clf_rf.feature_importances_, index=input_columns)
    plt.figure(figsize = (15,8))
    feat_importances.nlargest(20).plot(kind='barh')
def giver_preprocessing(giver,y):
    final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)
    temp = pd.pivot_table(giver,index = ['accountid','broken_time_end'],values = 'GiveNum',aggfunc = ['mean','sum','max','min'])
    temp.columns = ['mean_givenum','sum_givenum','max_givenum','min_givenum']
    temp = temp.add_prefix('giver_')
    temp = temp.reset_index()
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    final_table.fillna(0,inplace=True)
    return final_table
def receiver_preprocessing(giver,y):
    final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)
    temp = pd.pivot_table(giver,index = ['accountid','broken_time_end'],values = 'GiveNum',aggfunc = ['mean','sum','max','min'])
    temp.columns = ['mean_givenum','sum_givenum','max_givenum','min_givenum']
    temp = temp.add_prefix('receiver_')
    temp = temp.reset_index()
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')
    final_table.fillna(0,inplace=True)
    return final_table
    
def golditem_preprocessing(golditem,y):
    
    # 處理欄位 NA 資料
    golditem.fillna(0,inplace=True)

    # FillNa
    golditem['en_cate'] = golditem['en_cate'].fillna('OTHER')

    # Drop Column
    golditem = golditem.drop(['itemname','logtype','userclienttype'], axis=1)

    ## Y結果表, Golditem 資料表串接
    output = golditem.copy()
    final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)

     # 將類別型欄位轉成 str type(方便後面 one hot)
    oneHotCol = ['Group']
    output[oneHotCol] = output[oneHotCol].astype(str)

    # One-Hot Encoding
    output_dum = pd.get_dummies(output[oneHotCol])
    output = pd.concat([output,output_dum], axis=1)

    # amount 帳戶欄位結果進行 4 則運算
    temp = pd.pivot_table(golditem,index = ['accountid','broken_time_end'],values = 'amount',aggfunc = ['mean','sum','max','min'])
    temp.columns = ['mean_amonut','sum_amonut','max_amonut','min_amonut']
    temp = temp.reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')

    # preamount 帳戶欄位結果進行 4 則運算
    temp = pd.pivot_table(golditem,index = ['accountid','broken_time_end'],values = 'preamount',aggfunc = ['mean','sum','max','min'])
    temp.columns = ['mean_preamonut','sum_preamonut','max_preamonut','min_preamonut']
    temp = temp.reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')

    # Group 帳戶欄位結果進行 One-hotEncoding
    temp = pd.get_dummies(data = golditem[['accountid','broken_time_end','Group']],columns = ['Group'])
    temp = pd.pivot_table(temp,index = ['accountid','broken_time_end'],aggfunc = 'sum').add_prefix('sum_').reset_index()
    temp['broken_time_end'] = pd.to_datetime(temp['broken_time_end'])
    final_table = pd.merge(final_table,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],how = 'left')

    return output, final_table
    
def get_former_n_index(array,n):
    return sorted(np.argsort(array)[-n:])

login_log = pd.read_csv("/content/drive/MyDrive/0627/login_log_for_0116_reserve_brokentime.csv",index_col = 'Unnamed: 0')
y = pd.read_csv("/content/drive/MyDrive/0627/broken_time_list_15mins_202206231438.csv")
y['broken_time_end'] = pd.to_datetime(y['broken_time_end'] )

login_log = login_log_preprocessing(login_log,y)
df_login = pd.merge(login_log,y,left_on=['accountid','broken_time_end'],right_on=['accountid','broken_time_end'], how="left")

input_columns = df_login.drop(['broken_time_end', 'broken_time_start','accountid','broken_time_clickbrokenpackage_end','isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()

X  = df_login[input_columns].values
y1 = df_login.isclick.values

X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

plot_importance(clf_rf,input_columns)

selected_col_from_login = []
for col in get_former_n_index(clf_rf.feature_importances_,5):
    selected_col_from_login.append(input_columns[col])
selected_col_from_login

"""----"""

charge = pd.read_csv("/content/drive/MyDrive/0627/charge_for_0116_reserve_brokentime.csv")
charge = charge_preprocessing(charge,y)
df_charge = pd.merge(charge,y,left_on=['accountid','broken_time_end'],right_on=['accountid','broken_time_end'], how="left")

X  = df_charge.drop(['broken_time_end', 'broken_time_start','accountid','broken_time_clickbrokenpackage_end','isclick','sum_click','sum_click_charge_money'],axis=1).values
y1 = df_charge.isclick.values
y2 = df_charge.sum_click.values
y3 = df_charge.sum_click_charge_money.values


X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

input_columns = df_charge.drop(['broken_time_end', 'broken_time_start','accountid','broken_time_clickbrokenpackage_end','isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()


selected_col_from_charge = []
for col in get_former_n_index(clf_rf.feature_importances_,5):
    selected_col_from_charge.append(input_columns[col])
print(f'selected col: {selected_col_from_charge}')

plot_importance(clf_rf,input_columns)

"""----
# charge and login
"""

df_join = pd.merge(df_login,df_charge,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_login','_charge'])

input_columns = df_join.drop(['isclick_login','isclick_charge','sum_click_login','sum_click_charge','broken_time_clickbrokenpackage_end_charge','broken_time_clickbrokenpackage_end_login','sum_click_charge_money_login','sum_click_charge_money_charge','accountid','broken_time_end','broken_time_start_login','broken_time_start_charge'],axis=1).columns.tolist()
X  = df_join[input_columns].values

y1 = df_join.isclick_login.values

X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

plot_importance(clf_rf,input_columns)

BrokenPlayerDepositClickTarget_data = y
# 讀 depositclick 相關數據(X)
BrokenPlayerDepositClick = pd.read_csv('/content/drive/MyDrive/0627/depositclicklog_0116_before_34hrs.csv')
# 讀 gameinfo 相關數據(X)
BrokenPlayerGameInfo_data = pd.read_csv('/content/drive/MyDrive/0627/tmd_gameinfo_playresult_broken_duration_0123_202206241750.csv')     # 讀破產玩家遊戲行為數據(破產前34hr)
Themeid_to_cate_data = pd.read_csv('/content/drive/MyDrive/0627/themeid_to_cate.csv')[['themeid','en_cate']]     # 讀遊戲對應表數據
Themeid_to_cate_data.rename(columns = {'themeid':'Themeid'},inplace = True)
# 前處理
# broken click target 
BrokenClickTarget = BrokenPlayerDepositClickTarget_ETL(BrokenPlayerDepositClickTarget_data)
# broken depositclick 特徵提取(逐筆 vs 統計量)
BrokenDepositClick_series, BrokenDepositClick_static = BrokenPlayer_DepositClick_FeatureExtraction(BrokenPlayerDepositClick)
# broken gameinfo 特徵提取(逐筆 vs 統計量)
BrokenGameInfo_series, BrokenGameInfo_static = BrokenPlayer_GameInfo_FeatureExtraction(BrokenPlayerGameInfo_data, Themeid_to_cate_data)

# Target, DepositClick, gameinfo 資料合併
df = pd.merge(BrokenClickTarget, BrokenDepositClick_static,left_on=['accountid','broken_time_end'], right_on=['accountid','broken_time_end'], how="left")
df = pd.merge(df, BrokenGameInfo_static,left_on=['accountid','broken_time_end'], right_on=['accountid','broken_time_end'], how="left")
df = df.fillna(0).reset_index().drop('index', axis=1)
#df = df.drop(['accountid','broken_time_end', 'broken_time_start', 'broken_time_clickbrokenpackage_end'], axis=1)

input_columns = df.drop(['accountid','broken_time_end','broken_time_start','broken_time_clickbrokenpackage_end','isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()
X = df[input_columns]
y1 = df.isclick

X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

selected_col_from_deposit_and_gameinfo = []
for col in get_former_n_index(clf_rf.feature_importances_,10):
    selected_col_from_deposit_and_gameinfo.append(input_columns[col])
print(f'selected col: {selected_col_from_deposit_and_gameinfo}')


plot_importance(clf_rf,input_columns)

"""----
# depositclick and login and charge
"""

temp = df_join
temp['broken_time_end'] = temp['broken_time_end'].astype(str).str[:19]
df_merge = pd.merge(temp,df,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'])

X = df_merge.drop(['accountid','broken_time_end','broken_time_start_charge','broken_time_start_login','broken_time_clickbrokenpackage_end_charge','isclick_charge','sum_click_charge','sum_click_charge_money_charge','broken_time_start', 'broken_time_clickbrokenpackage_end', 'isclick','sum_click','sum_click_charge_money', 'broken_time_clickbrokenpackage_end_login', 'isclick_login','sum_click_login', 'sum_click_charge_money_login', ],axis=1).values
y1 = df_merge.isclick.values

X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

input_columns = df_merge.drop(['accountid','broken_time_end','broken_time_start_charge','broken_time_start_login','broken_time_clickbrokenpackage_end_charge','isclick_charge','sum_click_charge','sum_click_charge_money_charge','broken_time_start', 'broken_time_clickbrokenpackage_end', 'isclick','sum_click','sum_click_charge_money', 'broken_time_clickbrokenpackage_end_login', 'isclick_login','sum_click_login', 'sum_click_charge_money_login', ],axis=1).columns.tolist()


plot_importance(clf_rf,input_columns)

"""----
# golditem and giftlog
"""

giftlog = pd.read_csv('/content/drive/MyDrive/0627/giftlog_for_0116_reserve_brokentime.csv')
#golditem = pd.read_csv('/content/drive/MyDrive/0627/golditem_for_0116_reserve_brokentime.csv')
golditem = pd.read_csv('/content/drive/MyDrive/0627/view_golditem_summary_join_table_202207071824.csv')

final_table = y.groupby(['accountid','broken_time_end']).size().reset_index().drop(0,axis=1)
giftlog['broken_time_end'] =pd.to_datetime(giftlog['broken_time_end']) 
#把 giver and receiver 分開分析
giver = pd.merge(final_table,giftlog,left_on=['accountid','broken_time_end'],right_on=['AccountIdForGiver','broken_time_end'], how="left").dropna()
receiver = pd.merge(final_table,giftlog,left_on=['accountid','broken_time_end'],right_on=['AccountIdForReceiver','broken_time_end'], how="left").dropna()

giver_processed = giver_preprocessing(giver,y)
receiver_processed = receiver_preprocessing(receiver,y)

df_giver = pd.merge(giver_processed,y,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_login','_charge'])
df_giver = df_giver.drop(['broken_time_start','broken_time_clickbrokenpackage_end','accountid','broken_time_end'],axis=1)
X = df_giver.drop(['isclick','sum_click','sum_click_charge_money'],axis=1).values
y1 = df_giver.isclick

X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

input_columns = df_giver.drop(['isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()

selected_col_from_giver = []
for col in get_former_n_index(clf_rf.feature_importances_,2):
    selected_col_from_giver.append(input_columns[col])
print(f'selected col: {selected_col_from_giver}')


plot_importance(clf_rf,input_columns)

df_receiver = pd.merge(receiver_processed,y,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_login','_charge'])
df_receiver = df_receiver.drop(['broken_time_start','broken_time_clickbrokenpackage_end','accountid','broken_time_end'],axis=1)

X = df_receiver.drop(['isclick','sum_click','sum_click_charge_money'],axis=1).values
y1 = df_receiver.isclick
X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

input_columns = df_receiver.drop(['isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()

selected_col_from_receiver = []
for col in get_former_n_index(clf_rf.feature_importances_,2):
    selected_col_from_receiver.append(input_columns[col])
print(f'selected col: {selected_col_from_receiver}')


plot_importance(clf_rf,input_columns)

_,golditem_processed = golditem_preprocessing(golditem,y)

df_golditem = pd.merge(golditem_processed,y,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_golditem','_y'])
df_golditem = df_golditem.drop(['broken_time_start','broken_time_clickbrokenpackage_end','accountid','broken_time_end'],axis=1)


input_columns = df_golditem.drop(['isclick','sum_click','sum_click_charge_money'],axis=1).columns.tolist()
X = df_golditem[input_columns].values
y1 = df_golditem.isclick
X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)


selected_col_from_golditem = []
for col in get_former_n_index(clf_rf.feature_importances_,5):
    selected_col_from_golditem.append(input_columns[col])
print(f'selected col: {selected_col_from_golditem}')


plot_importance(clf_rf,input_columns)

"""----
## login and golditem
"""

temp = pd.merge(golditem_processed,y,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_golditem','_y'])
df_join = pd.merge(df_login,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'])
df_join

df_join = df_join.drop(['broken_time_start_x','broken_time_start_y','broken_time_clickbrokenpackage_end_x','broken_time_clickbrokenpackage_end_y','accountid','broken_time_end'],axis=1)
X = df_join.drop(['isclick_x','isclick_y','sum_click_x','sum_click_y','sum_click_charge_money_x','sum_click_charge_money_y'],axis=1).values
y1 = df_join.isclick_x
X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

"""----
# login and golditem and depositclick
"""

temp = pd.merge(golditem_processed,y,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'],suffixes=['_golditem','_y'])
temp = pd.merge(df_login,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'])
temp['broken_time_end'] = temp['broken_time_end'].astype(str)
temp['broken_time_end'] = temp['broken_time_end'].str[:19]
df_join = pd.merge(BrokenDepositClick_static,temp,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'])
df_join

df_join = df_join.drop(['broken_time_start_x','broken_time_start_y','broken_time_clickbrokenpackage_end_x','broken_time_clickbrokenpackage_end_y','accountid','broken_time_end'],axis=1)
X = df_join.drop(['isclick_x','isclick_y','sum_click_x','sum_click_y','sum_click_charge_money_x','sum_click_charge_money_y'],axis=1).values
y1 = df_join.isclick_x
X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

"""----
# 調參與join成一張大表
"""

for var in list(filter(lambda x : 'selected' in x ,list(globals().keys()))):
    
    eval(f"{var}.extend(['accountid', 'broken_time_end'])")
    exec(f"{var} = np.unique({var}).tolist()")

login_before_concat = login_log[selected_col_from_login]
charge_before_concat = charge[selected_col_from_charge]
depositclick_and_gameinfo_before_concat = df[selected_col_from_deposit_and_gameinfo]
giver_before_concat = giver_processed[selected_col_from_giver]
receiver_before_concat = receiver_processed[selected_col_from_receiver]
golditem_beofre_conct = golditem_processed[selected_col_from_golditem]

for var in list(filter(lambda x : 'before_concat' in x ,list(globals().keys()))):  
    a = f"{var}['broken_time_end'] = {var}['broken_time_end'].astype(str)"
    exec(a)
    a = f"{var}['broken_time_end'] = {var}['broken_time_end'].str[:19]"
    exec(a)

preconcated_df = []
for _df_ in list(filter(lambda x : 'before_concat' in x ,list(globals().keys()))):
    eval(f"{_df_}.set_index(['accountid','broken_time_end'],inplace = True,drop = True)")
    eval(f"preconcated_df.append({_df_})")


preconcated_df.append(temp)
preconcated_df

training_df = pd.concat(preconcated_df,axis=1)
training_df.reset_index(inplace =True)

training_df = pd.merge(BrokenClickTarget,training_df,left_on = ['accountid','broken_time_end'],right_on = ['accountid','broken_time_end'])
training_df

input_columns = training_df.drop([
                                  'accountid',
                                  'broken_time_end',
                                  'isclick_x',
                                  'isclick_y',
                                  'sum_click_charge_money_x',
                                  'sum_click_charge_money_y',
                                  'sum_click_x',
                                  'sum_click_y',
                                  'broken_time_start_x',
                                  'broken_time_start_y',
                                  'broken_time_clickbrokenpackage_end_x',
                                  'broken_time_clickbrokenpackage_end_y',
                                  ],axis=1).columns.tolist()
X = training_df[input_columns].reset_index(drop = True)
y1 = training_df.isclick_x
X_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train,'SMOTETomek')
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv= train_classifier_model(X_train, X_test, Y_train, Y_test)

!pip install shap
import shap
tree_model_explainer(clf_xgb, X_test)

from datetime import datetime
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold

def timer(start_time=None):
    if not start_time:
        start_time = datetime.now()
        return start_time
    elif start_time:
        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)
        tmin, tsec = divmod(temp_sec, 60)
        print('\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))
params = {
        'min_child_weight': [1, 5, 10,15,20,30],
        'gamma': [0.5, 1, 1.5, 2, 5,8],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0,1.2],
        'max_depth': [3, 4, 5,6,7,8,9,10],
        'n_estimators':[300,600,900,1200,1500]
        }
folds = 3
param_comb = 5

xgb = XGBClassifier(learning_rate=0.02,  objective='binary:logistic',
                    silent=True, nthread=4)

skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)

random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,Y_train), verbose=3, random_state=1001 )

# Here we go
start_time = timer(None) # timing starts from this point for "start_time" variable
random_search.fit(X_train, Y_train)
timer(start_time)

print('\n All results:')
print(random_search.cv_results_)
print('\n Best estimator:')
print(random_search.best_estimator_)
print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(random_search.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(random_search.best_params_)
results = pd.DataFrame(random_search.cv_results_)
results.to_csv('xgb-random-grid-search-results-01.csv', index=False)

xgb =XGBClassifier(colsample_bytree=1.0, gamma=1.5, learning_rate=0.02, max_depth=4,
              min_child_weight=5, n_estimators=600, nthread=4, silent=True,
              subsample=0.6).fit(X_train,Y_train)

tree_model_explainer(xgb, X_test)

def Ensemble_Stacking(X_train, X_test, Y_train, Y_test, list_model:list, final_model):

    #設立模型參數
    clf = StackingClassifier(estimators=list_model, final_estimator=final_model)

    #訓練 Stacking 模型
    model = clf.fit(X_train, Y_train)

    #model.score
    classifier_score(model, X_test, Y_test)
    accuracy, precision, recall, f1, roc_auc = classifier_score(model, X_test, Y_test)
    print(f'stacking => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    print("Stack_Model_Score : "+str(model.score(X_test, Y_test)))
    print("Estimators_Model : "+str(list_model))
    print("Final_Estimator_Model : "+str(final_model))

    
    return model
# Model List
estimators = [
    ('decisiontree', DecisionTreeClassifier(max_depth=8)),
    ('randomforest', RandomForestClassifier()),
    ('XGBClassifier', XGBClassifier(colsample_bytree=1.0, gamma=1.5, learning_rate=0.02, max_depth=4,
              min_child_weight=5, n_estimators=600, nthread=4, silent=True,
              subsample=0.6)),
    ('ExtraTreesClassifier',ExtraTreesClassifier()),
    ('GradientBoostingClassifier',GradientBoostingClassifier())
]

#產生Stacking Model
Stacking_Model = Ensemble_Stacking(X_train, X_test, Y_train, Y_test, estimators, AdaBoostClassifier())

"""----"""

