# -*- coding: utf-8 -*-
"""test_for_kmers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nsx-U_s5eDenzGRcMLr4_dBaxsq7R7bp
"""

!pip install shap

# 相關套件
import pymongo

import pandas as pd
import numpy as np
import os
import warnings

import time
from datetime import timedelta
from datetime import datetime

import matplotlib.pyplot as plt
import plotly.graph_objects as go
import seaborn as sns
from pandas.plotting import radviz, andrews_curves, parallel_coordinates

from imblearn.over_sampling import SMOTE, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingRegressor, StackingClassifier
from sklearn.svm import SVC, NuSVC, LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier

from xgboost import XGBClassifier, XGBRFRegressor, plot_importance

from lightgbm import LGBMClassifier, LGBMRegressor

#from lazypredict.Supervised import LazyClassifier, LazyRegressor

import shap

from tensorflow.keras import backend as K # This is the most common usecase for keras using K
from keras.models import Sequential
from keras.layers import Dense,Flatten,Activation

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)

# 資料不平衡處理(UnderSample,SMOTE,BorderlineSMOTE,SMOTETomek)
def data_balance(X_train, Y_train, method='SMOTE'):

    print('data balance by:')

    if method == 'UnderSample':
        # under sampling
        print('UnderSample')
        X_under, Y_under = RandomUnderSampler(sampling_strategy='majority').fit_resample(X_train, Y_train)
        # output = pd.concat([X_under, Y_under], axis=1)
        return X_under, Y_under

    elif method == 'SMOTE':
        # SMOTE
        # 設定一個近鄰值 K ，針對該樣本找出 K 個最近鄰樣本並從中隨機選一個
        # 設定一個採樣倍率 N，也就是對每個樣本需要生成幾個合成樣本並根據公式生成
        print('SMOTE')
        X_smo, Y_smo = SMOTE(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_smo, Y_smo], axis=1)
        return X_smo, Y_smo

    elif method == 'BorderlineSMOTE':
        # BorderlineSMOTE
        # 避免"跟多數樣本混合在一起的少數樣本"過多的學習而產生噪音
        print('BorderlineSMOTE')
        X_blsmo, Y_blsmo = BorderlineSMOTE(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_blsmo, Y_blsmo], axis=1)
        return X_blsmo, Y_blsmo

    elif method == 'SMOTETomek':
        # SMOTETomek
        # 先 SMOTE 過取樣再欠取樣
        print('SMOTETomek')
        X_smot, Y_smot = SMOTETomek(random_state=42).fit_resample(X_train, Y_train)
        # output = pd.concat([X_smot, Y_smot], axis=1)
        return X_smot, Y_smot

# classifier 模型分數
def classifier_score(model, X_test, Y_test):

    accuracy = round(accuracy_score(model.predict(X_test), Y_test),2)
    precision = round(precision_score(model.predict(X_test), Y_test, average='macro'),2)
    recall = round(recall_score(model.predict(X_test), Y_test, average='macro'),2)
    f1 = round(f1_score(model.predict(X_test), Y_test, average='macro'),2)
    roc_auc = round(roc_auc_score(model.predict(X_test), Y_test),2)

    return accuracy, precision, recall, f1, roc_auc

# 建常見分類模型
def train_classifier_model(X_train, X_test, Y_train, Y_test, lazy_predict=0):

    # lazy predict 初步看結果
    if lazy_predict == 1:
        models, predictions = LazyClassifier().fit(X_train, X_test, Y_train, Y_test)
        print(models)
    else:
        pass

    # Classifier model
    print('models:')
    # decisiontree
    clf_dt = DecisionTreeClassifier(max_depth=8)
    clf_dt.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_dt, X_test, Y_test)
    print(f'decisiontree => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # randomforest
    clf_rf = RandomForestClassifier()
    clf_rf.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc= classifier_score(clf_rf, X_test, Y_test)
    print(f'randomforest => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # xgboost
    clf_xgb = XGBClassifier()
    clf_xgb.fit(X_train,Y_train, verbose=False)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_xgb, X_test, Y_test)
    print(f'xgboost => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # extratrees
    clf_ext = ExtraTreesClassifier()
    clf_ext.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_ext, X_test, Y_test)
    print(f'extratrees => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # adaboost
    clf_adb = AdaBoostClassifier()
    clf_adb.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_adb, X_test, Y_test)
    print(f'adaboost => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # lgbm
    clf_lgbm = LGBMClassifier()
    clf_lgbm.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_lgbm, X_test, Y_test)
    print(f'lgbm => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # gradientboosting
    clf_gdb = GradientBoostingClassifier()
    clf_gdb.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_gdb, X_test, Y_test)
    print(f'gradientboosting => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # bagging
    clf_bag = BaggingClassifier()
    clf_bag.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_bag, X_test, Y_test)
    print(f'bagging => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # knn
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_knn, X_test, Y_test)
    print(f'knn => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')
    # NuSVC
    clf_nsv = NuSVC()
    clf_nsv.fit(X_train,Y_train)
    accuracy, precision, recall, f1, roc_auc = classifier_score(clf_nsv, X_test, Y_test)
    print(f'nusvc => accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}, roc_auc:{roc_auc}')

    return clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv 

# 樹模型解釋器
def tree_model_explainer(model, df):

    shap.initjs

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(df)
    print('expect value', explainer.expected_value)

    # 輸出 feature importance 排序
    feature_importance = pd.DataFrame()
    feature_importance['feature'] = df.columns
    feature_importance['importance'] = np.abs(shap_values).mean(0)
    feature_importance = feature_importance.sort_values('importance', ascending=False)

    # shap value 絕對值排名
    shap.summary_plot(shap_values, df, plot_type='bar')

    # shap value
    shap.summary_plot(shap_values, df, plot_type='dot')

    # 分別看特徵值與預測影響之關係圖(top5)
    feature_importance_top5 = feature_importance[:5]['feature'].to_list()
    print('feature_importance_top5', feature_importance_top5)
    for feature in feature_importance_top5:
        shap.dependence_plot(ind=feature,shap_values=shap_values, features=df, interaction_index=None)

    # top2 特徵值與預測影響之關係圖交互影響
    shap.dependence_plot(ind=feature_importance_top5[0],shap_values=shap_values, features=df, interaction_index=feature_importance_top5[1])
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

two_mers = pd.read_csv('2mers.csv')
three_mers = pd.read_csv('3mers.csv')

two_mers

predict_col_name = 'level_change_2way'

X = two_mers.drop(predict_col_name,axis=1)
Y = two_mers.loc[:,predict_col_name]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train, 'SMOTETomek')

# modeling
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv = train_classifier_model(X_train, X_test, Y_train, Y_test, lazy_predict=0)
# model explain
tree_model_explainer(clf_xgb, X_test)

"""





這邊加上reshape的code







"""


# keras CNN model (with conv1d)
# level_change_2way 分類

# 時間步長
n_timesteps = train_x.shape[1]
# x 輸入數量
n_features = train_x.shape[2]
# y 輸出數量
n_outputs = train_y.shape[1]
# 迭代次數
epochs_num = 60
# 每次訓練數
batch_size_set = 500
# 訓練過程打印
verbose_set = 0

# 建構 CNN model
model = Sequential()
model.add(Dense(20, activation='relu',input_shape = (train_x.shape[1],train_x.shape[2])))
model.add(Flatten())
model.add(Dense(30, activation='relu'))
model.add(Dense(units=n_outputs))
model.add(Activation("softmax")) # for class
# model 判斷標準
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall_m])
# print(model.summary())

# train model
model.fit(train_x, train_y, epochs=epochs_num, batch_size=batch_size_set, verbose=verbose_set)

# evaluate score
loss, accuracy, recall = model.evaluate(test_x, test_y, verbose=0)
print(f'acc:{round(accuracy,5)}, recall:{round(recall,5)}')

three_mers.fillna(0,inplace=True)
X = three_mers.drop(predict_col_name,axis=1)
Y = three_mers.loc[:,predict_col_name]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=150)
X_train, Y_train = data_balance(X_train, Y_train, 'SMOTETomek')

# modeling
clf_dt, clf_rf, clf_xgb, clf_ext, clf_adb, clf_lgbm, clf_gdb, clf_bag, clf_knn, clf_nsv = train_classifier_model(X_train, X_test, Y_train, Y_test, lazy_predict=0)
# model explain
tree_model_explainer(clf_xgb, X_test)

"""





這邊加上reshape的code







"""
# keras CNN model (with conv1d)
# level_change_2way 分類

# 時間步長
n_timesteps = train_x.shape[1]
# x 輸入數量
n_features = train_x.shape[2]
# y 輸出數量
n_outputs = train_y.shape[1]
# 迭代次數
epochs_num = 60
# 每次訓練數
batch_size_set = 500
# 訓練過程打印
verbose_set = 0

# 建構 CNN model
model = Sequential()
model.add(Dense(20, activation='relu'))
model.add(Flatten())
model.add(Dense(30, activation='relu'))
model.add(Dense(units=n_outputs))
model.add(Activation("softmax")) # for class
# model 判斷標準
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall_m])
# print(model.summary())

# train model
model.fit(train_x, train_y, epochs=epochs_num, batch_size=batch_size_set, verbose=verbose_set)

# evaluate score
loss, accuracy, recall = model.evaluate(test_x, test_y, verbose=0)
print(f'acc:{round(accuracy,5)}, recall:{round(recall,5)}')